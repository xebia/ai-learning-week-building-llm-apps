{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building your first RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a hands-on first experience on **Retrieval-Augmented Generation (RAG)** using the `google-genai` Python SDK, configured for **Vertex AI**.\n",
    "\n",
    "**Why RAG?**\n",
    "*   **Reduces Hallucinations:** Provides factual context.\n",
    "*   **Uses Up-to-Date Information:** Accesses newer data.\n",
    "*   **Domain-Specific Knowledge:** Handles private/specialized docs.\n",
    "\n",
    "**The Core RAG Process:**\n",
    "1.  **Query:** User asks a question.\n",
    "2.  **Retrieve:** System finds relevant documents.\n",
    "3.  **Augment:** Query + Retrieved Docs = New Prompt.\n",
    "4.  **Generate:** LLM answers using the augmented prompt.\n",
    "\n",
    "Let's build a basic RAG flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "import textwrap \n",
    "\n",
    "print(f\"google-genai SDK imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, configure the client for Vertex AI. \n",
    "\n",
    "**Note:** This assumes your environment (e.g., a Vertex AI Notebook, or a local environment where you've run `gcloud auth application-default login`) is already authenticated to use Google Cloud services. Execution will fail below if Project ID or Location are incorrect or if authentication is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Configure these values for your environment --- \n",
    "PROJECT_ID = \"...\" # Your Google Cloud Project ID\n",
    "LOCATION = \"...\"   # The region for Vertex AI services, e.g., \"us-central1\"\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Attempt to initialize the client - this will fail if config is invalid/missing\n",
    "print(f\"Configuring client for Project: {PROJECT_ID}, Location: {LOCATION}\")\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
    "print(\"✅ google-genai client configured for Vertex AI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preparation & Indexing\n",
    "\n",
    "We need a knowledge base. We'll convert these texts into **embeddings**.\n",
    "\n",
    "> #### Exercise 📝\n",
    "> Choose an appropriate embedding model name available on Vertex AI. See [Vertex AI documentation for text embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#text_embedding_models). If the name is invalid, the next cell will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our simple knowledge base\n",
    "documents = [\n",
    "    \"Mercury is the smallest planet in our Solar System and nearest to the Sun.\",\n",
    "    \"Venus is the second planet from the Sun and is Earth's closest planetary neighbor.\",\n",
    "    \"Earth is the third planet from the Sun and the only astronomical object known to harbor life.\",\n",
    "    \"Mars is the fourth planet from the Sun and the second-smallest planet, often called the 'Red Planet'.\",\n",
    "    \"Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant.\",\n",
    "    \"Saturn is the sixth planet from the Sun, famous for its prominent ring system. It's another gas giant.\",\n",
    "    \"Uranus is the seventh planet from the Sun. It has a unique tilt, making it rotate nearly on its side.\",\n",
    "    \"Neptune is the eighth and farthest known planet from the Sun. It's a dark, cold world.\"\n",
    "]\n",
    "\n",
    "# --- TODO: Specify the embedding model name --- \n",
    "embedding_model_name = \"...\"\n",
    "# --------------------------------------------\n",
    "\n",
    "print(f\"Using embedding model: {embedding_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings. This cell will fail if the client/model is not set up correctly, or if the API call fails, or if the response doesn't contain `['embedding']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = [] # To store {text:, embedding:} dictionaries\n",
    "\n",
    "print(f\"Generating embeddings for {len(documents)} documents...\")\n",
    "    \n",
    "# Call embed_content - Errors will now halt execution and display here\n",
    "response: types.EmbedContentResponse = client.models.embed_content(\n",
    "    model=embedding_model_name, \n",
    "    contents=documents \n",
    ")\n",
    "\n",
    "# Directly access the embeddings - this assumes response['embedding'] exists and is a list\n",
    "embeddings_list: list[list[float]] = response.embeddings\n",
    "\n",
    "# Store embeddings along with the original text\n",
    "for doc, embedding in zip(documents, response.embeddings):\n",
    "    document_embeddings.append({\n",
    "        \"text\": doc,\n",
    "        # Convert to numpy array for easier math later\n",
    "        \"embedding\": np.array(embedding.values) \n",
    "    })\n",
    "    \n",
    "print(f\"✅ Successfully generated and processed {len(document_embeddings)} embeddings.\")\n",
    "# print(f\"Sample embedding dimension: {document_embeddings[0]['embedding'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 'index' is now ready. Real systems use specialized **vector databases** (like Vertex AI Matching Engine) for efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Retrieval\n",
    "\n",
    "To find relevant documents for a user query:\n",
    "1.  Embed the query.\n",
    "2.  Calculate similarity.\n",
    "3.  Select top-k.\n",
    "\n",
    "> #### Exercise 📝\n",
    "> Complete the `calculate_cosine_similarity` function using numpy operations (`np.dot`, `np.linalg.norm`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def embed_query(query_text: str) -> np.ndarray:\n",
    "    \"\"\"Generates an embedding for a single query string.\"\"\"\n",
    "    # Call embed_content for the query - will fail if client/model invalid\n",
    "    response: types.EmbedContentResponse = client.models.embed_content(\n",
    "        model=embedding_model_name,\n",
    "        contents=query_text \n",
    "    )\n",
    "    # Directly access the embedding - assumes response['embedding'] exists and is a list\n",
    "    return np.array(response.embeddings[0].values) # Convert to numpy array for easier math later\n",
    "\n",
    "def calculate_cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Calculates the cosine similarity between two numpy vectors.\"\"\"\n",
    "    # Convert just in case they are not arrays yet\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    # --- TODO: Implement Cosine Similarity --- \n",
    "    dot_product: float = ... # Dot product of vec1 and vec2\n",
    "    norm_vec1: float = ... # Magnitude (norm) of vec1\n",
    "    norm_vec2: float = ... # Magnitude (norm) of vec2\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # Check for zero vectors to avoid division by zero\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0 \n",
    "    \n",
    "    similarity: float = dot_product / (norm_vec1 * norm_vec2)\n",
    "    return similarity\n",
    "\n",
    "def retrieve_documents(query: str, num_documents_to_retrieve: int = 3) -> List[str]:\n",
    "    \"\"\"Retrieves the top_k most relevant documents for a given query.\"\"\"\n",
    "    # This will fail if document_embeddings list is empty or not defined\n",
    "    # This will fail if embed_query fails\n",
    "    print(f\"\\nRetrieving documents for query: '{query}'\")\n",
    "    query_embedding: np.ndarray = embed_query(query)\n",
    "\n",
    "    similarities: List[tuple[float, str]] = []\n",
    "    # This will fail if calculate_cosine_similarity fails (e.g., if query_embedding was None - though embed_query should have failed first)\n",
    "    for doc_data in document_embeddings:\n",
    "        similarity: float = calculate_cosine_similarity(query_embedding, doc_data['embedding'])\n",
    "        similarities.append((similarity, doc_data['text']))\n",
    "\n",
    "    # Sort by similarity \n",
    "    similarities.sort(key=lambda item: item[0], reverse=True)\n",
    "\n",
    "    # Slice for top k - will fail if num_documents_to_retrieve is not int > 0\n",
    "    retrieved_texts: List[str] = [text for similarity, text in similarities[:num_documents_to_retrieve]]\n",
    "    print(f\"Retrieved {len(retrieved_texts)} documents.\")\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test retrieval.\n",
    "\n",
    "> #### Exercise 📝\n",
    "> Choose how many documents (`top_k`) you want to retrieve. If k is invalid, the slicing in `retrieve_documents` might fail or behave unexpectedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Which planet is known for its rings?\"\n",
    "# --- TODO: Set the number of documents to retrieve --- \n",
    "k = ... # e.g., 2 or 3\n",
    "retrieve_documents(user_query, num_documents_to_retrieve=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was cool! We just retrieved the most relevant documents for our query.\n",
    "\n",
    "Let's try another couple queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents(\"chocolate\", num_documents_to_retrieve=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents(\"water\", num_documents_to_retrieve=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents(\"What is the smallest planet to orbit the sun in our solar system?\", num_documents_to_retrieve=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_documents(\"<your question>\", num_documents_to_retrieve=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise 📝\n",
    "> Why does each query return what it returns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Augmentation & Generation\n",
    "\n",
    "Combine query and retrieved documents into an augmented prompt.\n",
    "\n",
    "> #### Exercise 📝\n",
    "> Choose a generative model available on Vertex AI. See [Vertex AI documentation for Gemini models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini_models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = retrieve_documents(\n",
    "    \"Which planet is known for its rings?\",\n",
    "    num_documents_to_retrieve=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Specify the generation model name --- \n",
    "generation_model_name = \"...\" # e.g., \"gemini-1.5-flash-001\"\n",
    "# -----------------------------------------------\n",
    "\n",
    "print(f\"Using generation model: {generation_model_name}\")\n",
    "\n",
    "# Construct the augmented prompt - Assumes 'retrieved' is a list of strings from previous cell\n",
    "context = \"\\n\".join(retrieved) \n",
    "augmented_prompt = f\"Based ONLY on the following information:\\n--- START CONTEXT ---\\n{context}\\n--- END CONTEXT ---\\n\\nAnswer the question: {user_query}\"\n",
    "    \n",
    "print(\"\\n--- Augmented Prompt ---\")\n",
    "print(textwrap.fill(augmented_prompt, width=80))\n",
    "print(\"\\n--- Generating Response ---\")\n",
    "\n",
    "# Call generate_content - Will fail if client/model invalid or API error\n",
    "response = client.models.generate_content(\n",
    "    model=generation_model_name,\n",
    "    contents=augmented_prompt,\n",
    "    # Optional config: \n",
    "    # config=types.GenerateContentConfig(temperature=0.2) \n",
    ")\n",
    "        \n",
    "# Directly access the response text - Assumes response has .text attribute\n",
    "print(\"\\nModel Response:\")\n",
    "print(textwrap.fill(response.text, width=80))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Putting It All Together\n",
    "\n",
    "Let's wrap the RAG process in a single function (still with minimal checks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_rag(query, num_docs=3):\n",
    "    \"\"\"Performs the full RAG process: Retrieve -> Augment -> Generate.\"\"\"\n",
    "    \n",
    "    # 1. Retrieve - Will fail if retrieval fails internally\n",
    "    retrieved_docs = retrieve_documents(query, num_documents_to_retrieve=num_docs)\n",
    "            \n",
    "    print(\"--- Retrieved for RAG Function ---       \")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"{i+1}. {textwrap.fill(doc, width=80)}\")\n",
    "        \n",
    "    # 2. Augment\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    augmented_prompt = (\n",
    "        f\"Use the following pieces of context to answer the question at the end. \"\n",
    "        f\"If you don't know the answer from the context, just say that you don't know, don't try to make up an answer.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\n\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Augmented Prompt (RAG Function) ---\")\n",
    "    print(textwrap.fill(augmented_prompt, width=80))\n",
    "    \n",
    "    # 3. Generate - Will fail if generation fails\n",
    "    print(\"\\n--- Generating Response (RAG Function) ---\")\n",
    "    response = client.models.generate_content(\n",
    "        model=generation_model_name,\n",
    "        contents=augmented_prompt,\n",
    "    )\n",
    "    \n",
    "    # Directly access text - Assumes .text attribute exists\n",
    "    final_answer = response.text\n",
    "    print(\"\\nModel Response:\")\n",
    "    print(textwrap.fill(final_answer, width=80))\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise 📝\n",
    "> Ask the RAG system a question. If the query is invalid or causes issues, the `perform_rag` function will likely fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Ask a question --- \n",
    "final_query = \"...\" # e.g., \"Describe the largest planet mentioned.\"\n",
    "# ---------------------------\n",
    "\n",
    "final_answer = perform_rag(final_query, num_docs=2) \n",
    "print(\"\\n--- RAG Process Completed ---       \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### 🎁 Bonus exercises 📝\n",
    "> - **Different Data:** Replace the planet documents with your own text data.\n",
    "> - **Vector Databases:** Learn about [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview), Pinecone, ChromaDB, or FAISS. Why are they needed for scale?\n",
    "> - **Generation:** Explore different generative models. How do they differ in performance? Select different models in the `perform_rag` function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
